{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Target Tensor\n",
    "\n",
    "target = torch.FloatTensor([[.1, .2, .3],\n",
    "                            [.4, .5, .6],\n",
    "                            [.7, .8, .9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2360, 0.3948, 0.9065],\n",
      "        [0.3604, 0.5385, 0.3750],\n",
      "        [0.8618, 0.0517, 0.8717]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand_like(target)\n",
    "x.requires_grad = True ## gradient 업데이트 허용 !! (필수)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1183, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "## Loss 구하기\n",
    "\n",
    "loss = F.mse_loss(x, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th Loss : 0.07158425450325012\n",
      "tensor([[0.2058, 0.3515, 0.7718],\n",
      "        [0.3692, 0.5299, 0.4250],\n",
      "        [0.8258, 0.2180, 0.8780]], requires_grad=True)\n",
      "2-th Loss : 0.04330405220389366\n",
      "tensor([[0.1823, 0.3178, 0.6669],\n",
      "        [0.3761, 0.5233, 0.4639],\n",
      "        [0.7979, 0.3473, 0.8829]], requires_grad=True)\n",
      "3-th Loss : 0.026196278631687164\n",
      "tensor([[0.1640, 0.2916, 0.5854],\n",
      "        [0.3814, 0.5181, 0.4941],\n",
      "        [0.7761, 0.4479, 0.8867]], requires_grad=True)\n",
      "4-th Loss : 0.015847129747271538\n",
      "tensor([[0.1498, 0.2713, 0.5220],\n",
      "        [0.3855, 0.5141, 0.5177],\n",
      "        [0.7592, 0.5261, 0.8896]], requires_grad=True)\n",
      "5-th Loss : 0.009586538188159466\n",
      "tensor([[0.1387, 0.2554, 0.4726],\n",
      "        [0.3887, 0.5110, 0.5360],\n",
      "        [0.7460, 0.5870, 0.8919]], requires_grad=True)\n",
      "6-th Loss : 0.005799262318760157\n",
      "tensor([[0.1301, 0.2431, 0.4343],\n",
      "        [0.3912, 0.5085, 0.5502],\n",
      "        [0.7358, 0.6343, 0.8937]], requires_grad=True)\n",
      "7-th Loss : 0.0035081948153674603\n",
      "tensor([[0.1234, 0.2335, 0.4044],\n",
      "        [0.3932, 0.5066, 0.5613],\n",
      "        [0.7279, 0.6711, 0.8951]], requires_grad=True)\n",
      "8-th Loss : 0.0021222420036792755\n",
      "tensor([[0.1182, 0.2261, 0.3812],\n",
      "        [0.3947, 0.5052, 0.5699],\n",
      "        [0.7217, 0.6998, 0.8962]], requires_grad=True)\n",
      "9-th Loss : 0.0012838258408010006\n",
      "tensor([[0.1142, 0.2203, 0.3632],\n",
      "        [0.3959, 0.5040, 0.5766],\n",
      "        [0.7169, 0.7221, 0.8970]], requires_grad=True)\n",
      "10-th Loss : 0.0007766354829072952\n",
      "tensor([[0.1110, 0.2158, 0.3491],\n",
      "        [0.3968, 0.5031, 0.5818],\n",
      "        [0.7131, 0.7394, 0.8977]], requires_grad=True)\n",
      "11-th Loss : 0.0004698168486356735\n",
      "tensor([[0.1086, 0.2123, 0.3382],\n",
      "        [0.3975, 0.5024, 0.5858],\n",
      "        [0.7102, 0.7528, 0.8982]], requires_grad=True)\n",
      "12-th Loss : 0.00028420999296940863\n",
      "tensor([[0.1067, 0.2095, 0.3297],\n",
      "        [0.3981, 0.5019, 0.5890],\n",
      "        [0.7079, 0.7633, 0.8986]], requires_grad=True)\n",
      "13-th Loss : 0.0001719293068163097\n",
      "tensor([[0.1052, 0.2074, 0.3231],\n",
      "        [0.3985, 0.5015, 0.5914],\n",
      "        [0.7062, 0.7715, 0.8989]], requires_grad=True)\n",
      "14-th Loss : 0.00010400674364063889\n",
      "tensor([[0.1040, 0.2058, 0.3180],\n",
      "        [0.3988, 0.5011, 0.5933],\n",
      "        [0.7048, 0.7778, 0.8992]], requires_grad=True)\n",
      "15-th Loss : 6.291759200394154e-05\n",
      "tensor([[0.1031, 0.2045, 0.3140],\n",
      "        [0.3991, 0.5009, 0.5948],\n",
      "        [0.7037, 0.7827, 0.8993]], requires_grad=True)\n",
      "16-th Loss : 3.806128370342776e-05\n",
      "tensor([[0.1024, 0.2035, 0.3109],\n",
      "        [0.3993, 0.5007, 0.5960],\n",
      "        [0.7029, 0.7866, 0.8995]], requires_grad=True)\n",
      "17-th Loss : 2.30246896535391e-05\n",
      "tensor([[0.1019, 0.2027, 0.3085],\n",
      "        [0.3994, 0.5005, 0.5969],\n",
      "        [0.7023, 0.7896, 0.8996]], requires_grad=True)\n",
      "18-th Loss : 1.3928535736340564e-05\n",
      "tensor([[0.1015, 0.2021, 0.3066],\n",
      "        [0.3996, 0.5004, 0.5976],\n",
      "        [0.7018, 0.7919, 0.8997]], requires_grad=True)\n",
      "19-th Loss : 8.425882697338238e-06\n",
      "tensor([[0.1011, 0.2016, 0.3051],\n",
      "        [0.3997, 0.5003, 0.5981],\n",
      "        [0.7014, 0.7937, 0.8998]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-5 # 0.00001\n",
    "lr = 1.       # 0.1 (Learning Rate)\n",
    "iter_cnt = 0     # 반복 횟수\n",
    "\n",
    "\n",
    "while loss >= threshold:\n",
    "    iter_cnt += 1\n",
    "\n",
    "    loss.backward()  # gradients Calculration.\n",
    "                     # x.grad에 자동적으로 저장됨..!  (이떄 loss값은 Scalar값이어야함)\n",
    "\n",
    "    x = x - lr * x.grad\n",
    "\n",
    "    x.detach_()\n",
    "    x.requires_grad_(True)\n",
    "\n",
    "    loss = F.mse_loss(x, target)\n",
    "\n",
    "    print(f'{iter_cnt}-th Loss : {loss}')\n",
    "    print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([[1, 2],\n",
    "                        [3, 4]]).requires_grad_(True)\n",
    "\n",
    "## requires_grad 속성이 True인 텐서가 있을 떄 이 텐서가 들어간 연산의 결과인 텐서도 requires_grad 속성이 True로 저장됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- x1 ----\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]], grad_fn=<AddBackward0>)\n",
      "---- x2 ----\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]], grad_fn=<SubBackward0>)\n",
      "---- x3 ----\n",
      "tensor([[-3.,  0.],\n",
      "        [ 5., 12.]], grad_fn=<MulBackward0>)\n",
      "---- x4 ----\n",
      "14.0\n"
     ]
    }
   ],
   "source": [
    "x1 = x + 2\n",
    "print(f'---- x1 ----\\n{x1}')\n",
    "\n",
    "x2 = x - 2\n",
    "print(f'---- x2 ----\\n{x2}')\n",
    "\n",
    "x3 = x1 * x2\n",
    "print(f'---- x3 ----\\n{x3}')\n",
    "\n",
    "x4 = x3.sum()\n",
    "print(f'---- x4 ----\\n{x4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 4.],\n",
       "        [6., 8.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.,  0.],\n",
       "        [ 5., 12.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3.detach_()  # 메모리 해제 (그래프로부터 뗴어낼 수 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d61e67d4406f83661a218a7594034be74564666d0640d3900a3e99845865d0f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
